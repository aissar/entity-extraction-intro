{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544eda08",
   "metadata": {},
   "source": [
    "# Entity Extraction with Large Language Models (LLMs)\n",
    "\n",
    "This notebook provides guided examples for using **OpenAI's** **gpt-4o-mini** model to extract structured JSON outputs from unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fde66",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Import the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94728783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import Type, List\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialise the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95940d29",
   "metadata": {},
   "source": [
    "### Define a Reusable Helper Function for Entity Extraction\n",
    "This function will handle the logic for entity extraction. It accepts two inputs as outlined below.\n",
    "\n",
    "#### Inputs\n",
    "* text: corpus of text that we we want to extract information from\n",
    "* custom_class: schema definition for the desired structured output format\n",
    "\n",
    "Note: We use OpenAI's **gpt-4o-mini** model for this task; however, you should be able to use any model that supports structured outputs.\n",
    "\n",
    "#### Benefits\n",
    "There are many advantages of abstracting and centralising logic inside a reusable function, including:\n",
    "- Improved maintainability e.g., only need to update in one place if the model provider releases a change\n",
    "- Experimentation e.g., trial different prompt instructions, model versions, or even model providers without needing to update downstream code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc18628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_single_input(text: str, custom_class: Type[BaseModel], client: OpenAI = OpenAI()):\n",
    "    \"\"\"\n",
    "    Perform entity extraction on a text input and structure the output according to a provided pydantic model.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The text to extract entities from.\n",
    "    - custom_class (Type[BaseModel]): A pydantic model class defining the structure of the output.\n",
    "    \n",
    "    Returns:\n",
    "    - Instance of custom_class containing the structured output.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract entities from the following text and format the output according to the provided JSON schema:\n",
    "    \n",
    "    {text}\n",
    "    \n",
    "    Use the following JSON format:\n",
    "    {custom_class.model_json_schema()}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "            n=1,\n",
    "            response_format=custom_class\n",
    "        )\n",
    "\n",
    "        extracted_object = completion.choices[0].message.parsed\n",
    "\n",
    "        return extracted_object\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(\"The server could not be reached\")\n",
    "        print(e.__cause__)  # an underlying Exception, likely raised within httpx.\n",
    "        return e\n",
    "    except openai.RateLimitError as e:\n",
    "        print(\"A 429 status code was received; we should back off a bit.\")\n",
    "        return e\n",
    "    except openai.APIStatusError as e:\n",
    "        print(\"Another non-200-range status code was received\")\n",
    "        print(e.status_code)\n",
    "        print(e.response)\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fd0d1",
   "metadata": {},
   "source": [
    "### Practical Examples of Extracting Structured Outputs from Unstructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55ccde",
   "metadata": {},
   "source": [
    "#### Example 1: Analysing News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4724a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a general purpose class\n",
    "class GeneralPurposeEntity(BaseModel):\n",
    "    type: str = Field(..., description=\"Type of entity, e.g., 'person', 'organisation', 'location'\")\n",
    "    text: str = Field(..., description=\"Extracted entity text\")\n",
    "\n",
    "# Return a list, noting that our input data may contain many instances of entities\n",
    "class GeneralPurposeEntityList(BaseModel):\n",
    "    entities: List[GeneralPurposeEntity] = Field(..., description=\"List of extracted entities\")\n",
    "\n",
    "# Sample news article\n",
    "# Source: https://www.wired.com/story/tesla-cybercab-is-here/\n",
    "sample_text_general = \"\"\"\n",
    "    AARIAN MARSHALL | GEAR | OCT 10. 2024 11:24 PM\n",
    "    Tesla's Cybercab Is Here\n",
    "    At a livestreamed event this evening, Tesla CEO Elon Musk showed off the company's new Cybercab and shared some details about Tesla's plan to launch its own robotaxi service.\n",
    "    \"\"\"\n",
    "\n",
    "# Perform entity extraction using the GeneralPurposeEntityList class\n",
    "resp = extract_entities_from_single_input(sample_text_general, GeneralPurposeEntityList).model_dump()\n",
    "print(json.dumps(resp, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95232c5",
   "metadata": {},
   "source": [
    "#### Example 2: Analysing Customer Support Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the allowed values for severity\n",
    "class Severity(str, Enum):\n",
    "    low = \"low\"\n",
    "    moderate = \"moderate\"\n",
    "    high = \"high\"\n",
    "\n",
    "class CustomerSupportTranscriptEntity(BaseModel):\n",
    "    reported_by: str = Field(..., description=\"Name of the person who reported the issue. Default to 'Unspecified' if unknown.\")\n",
    "    issue_type: str = Field(..., description=\"Type of issue, e.g., 'battery', 'screen'\")\n",
    "    location: str = Field(..., description=\"The city associated with the issue.\")\n",
    "    severity: Severity = Field(..., description=\"Severity level of the issue. Categorise as 'low' if issue impacts a few people, 'moderate' if issue impacts 5-10 people, or 'high' if issue impacts more than 10 people.\")\n",
    "\n",
    "# Sample customer support transcript\n",
    "sample_text_customer_support = \"Annie Liao from BuildClub reported an issue with the air conditioning unit on Level 2 at the Stone & Chalk building located in Hay Market, Sydney.\"\n",
    "\n",
    "# Perform entity extraction using the CustomerSupportTranscriptEntity class\n",
    "resp = extract_entities_from_single_input(sample_text_customer_support, CustomerSupportTranscriptEntity).model_dump()\n",
    "print(json.dumps(resp, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3bd22",
   "metadata": {},
   "source": [
    "#### Example 3: Processing Multiple Inputs\n",
    "In the previous examples we dealt with single inputs. But what if we have many inputs e.g., many news articles or customer support transcripts? \n",
    "\n",
    "Let's create a wrapper around our helper function to accepts a list of inputs and iterate over each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_list_input(texts: List[str], custom_class: Type[BaseModel], client: OpenAI = OpenAI()):\n",
    "    results = []\n",
    "    # Iterate over each item and append the model output to results set\n",
    "    for text in texts:\n",
    "        result = extract_entities_from_single_input(text, custom_class, client).model_dump()\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# List of sample news articles\n",
    "sample_text_list = [\n",
    "    \"Customer Jack Wakem came into the Melbourne CBD Apple store requesting repair of Macbook Pro due to faulty battery.\",\n",
    "    \"Anonymous complaint in relation to students using mechnical keyboards, disturbing peace and quiet of other students at Sydney Uni library.\"\n",
    "]\n",
    "\n",
    "resp = extract_entities_from_list_input(sample_text_list, CustomerSupportTranscriptEntity)\n",
    "print(json.dumps(resp, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48909783",
   "metadata": {},
   "source": [
    "#### Prepare the Input File for Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0df1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a research paper class\n",
    "class ResearchPaperEntity(BaseModel):\n",
    "    title: str\n",
    "    authors: list[str]\n",
    "    abstract: str\n",
    "    keywords: list[str]\n",
    "\n",
    "# Function to pre process the sample input file\n",
    "def prepare_batch_input_file(input_file: str, output_file: str, custom_class: Type[BaseModel]):\n",
    "    \"\"\"\n",
    "    Reads a .jsonl file, adds extra fields to each entry, and writes it to a new .jsonl file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input .jsonl file.\n",
    "        output_file (str): Path to the output .jsonl file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Load the line as a JSON object (dictionary)\n",
    "            entry = json.loads(line.strip())\n",
    "            \n",
    "            # Append additional fields needed for the batch api\n",
    "            prompt = f\"\"\"\n",
    "            Extract entities from the following text and format the output according to the provided JSON schema:\n",
    "            \n",
    "            {entry['content']}\n",
    "            \n",
    "            Use the following JSON format:\n",
    "            {custom_class.model_json_schema()}\n",
    "            \"\"\"\n",
    "\n",
    "            extra_fields = {\n",
    "                \"custom_id\": entry['row_id'],\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": prompt},\n",
    "                        {\"role\": \"user\", \"content\": entry['content']}\n",
    "                    ],\n",
    "                    \"max_tokens\": 300,\n",
    "                    \"temperature\": 0,\n",
    "                    \"n\": 1,\n",
    "                    \"response_format\": custom_class.model_json_schema()\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Update the entry with additional fields\n",
    "            entry.update(extra_fields)\n",
    "\n",
    "            # Create a new entry with only the specified fields\n",
    "            filtered_entry = {field: entry[field] for field in extra_fields if field in entry}\n",
    "            \n",
    "            # Write the modified entry to the output file\n",
    "            outfile.write(json.dumps(filtered_entry) + '\\n')\n",
    "    \n",
    "    print(f\"Modified entries written to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = 'sample_research_papers.jsonl'\n",
    "output_file = 'batch_input.jsonl'\n",
    "custom_class = ResearchPaperEntity\n",
    "\n",
    "prepare_batch_input_file(input_file, output_file, custom_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09257434",
   "metadata": {},
   "source": [
    "#### Upload the Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_file = client.files.create(\n",
    "  file=open(\"batch_input.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e9533",
   "metadata": {},
   "source": [
    "#### Submit the Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch_job = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "      \"description\": \"entity extraction job\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e101a9e",
   "metadata": {},
   "source": [
    "#### Check the Batch Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(client.batches.retrieve(batch_job.id).model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79945a2d",
   "metadata": {},
   "source": [
    "#### Retrieve the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(batch_job.status == 'completed'):\n",
    "    file_response = client.files.content(batch_job.output_file_id)\n",
    "    print(file_response.text)\n",
    "else:\n",
    "    print(batch_job.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6bf53",
   "metadata": {},
   "source": [
    "### Custom Batch Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707408ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import asyncio\n",
    "import random\n",
    "from functools import partial\n",
    "from typing import Type, Callable, Any, List\n",
    "from pydantic import BaseModel, Field\n",
    "import openai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Use the async client\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Define a research paper class\n",
    "class ResearchPaperEntity(BaseModel):\n",
    "    paper_title: str = Field(..., description=\"Inferred from the header section or contents of the paper.\")\n",
    "    authors: list[str] = Field(..., description=\"Names of individuals who contributed to the paper.\")\n",
    "    abstract: str = Field(..., description=\"One line summary of the paper.\")\n",
    "    keywords: list[str] = Field(..., description=\"Metadata to assist with keyword search.\")\n",
    "\n",
    "# Helper function with retry and backoff mechanism\n",
    "async def backoff_retry_async(func: Callable[[], Any], max_retries=5, base_delay=1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return await func()\n",
    "        except openai.RateLimitError as e:\n",
    "            delay = base_delay * (2 ** retries) + random.uniform(0, 0.1)  # Exponential backoff with jitter\n",
    "            print(f\"Rate limit reached. Retrying in {delay:.2f} seconds...\")\n",
    "            await asyncio.sleep(delay)\n",
    "            retries += 1\n",
    "        except openai.APIConnectionError as e:\n",
    "            print(\"The server could not be reached\")\n",
    "            print(e.__cause__)\n",
    "            return e  # Return the exception if non-retry error occurs\n",
    "        except openai.APIStatusError as e:\n",
    "            print(\"Another non-200-range status code was received\")\n",
    "            print(e.status_code)\n",
    "            print(e.response)\n",
    "            return e  # Return the exception if non-retry error occurs\n",
    "\n",
    "    return openai.RateLimitError(\"Max retries exceeded. Exiting.\")  # Return a custom error after max retries\n",
    "\n",
    "# Updated entity extraction function using asynchronous client\n",
    "async def extract_entities_from_single_input_async(text: str, custom_class: Type[BaseModel], client: AsyncOpenAI = AsyncOpenAI()):\n",
    "    \"\"\"\n",
    "    Perform entity extraction on a text input and structure the output according to a provided pydantic model.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The text to extract entities from.\n",
    "    - custom_class (Type[BaseModel]): A pydantic model class defining the structure of the output.\n",
    "    \n",
    "    Returns:\n",
    "    - Instance of custom_class containing the structured output.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract entities from the following text and format the output according to the provided JSON schema:\n",
    "    \n",
    "    {text}\n",
    "    \n",
    "    Use the following JSON format:\n",
    "    {custom_class.model_json_schema()}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = await client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            max_tokens=1000,\n",
    "            temperature=0,\n",
    "            n=1,\n",
    "            response_format=custom_class\n",
    "        )\n",
    "\n",
    "        extracted_object = completion.choices[0].message.parsed\n",
    "\n",
    "        return extracted_object\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(\"The server could not be reached\")\n",
    "        print(e.__cause__)  # an underlying Exception, likely raised within httpx.\n",
    "        return e\n",
    "    except openai.RateLimitError as e:\n",
    "        print(\"A 429 status code was received; we should back off a bit.\")\n",
    "        return e\n",
    "    except openai.APIStatusError as e:\n",
    "        print(\"Another non-200-range status code was received\")\n",
    "        print(e.status_code)\n",
    "        print(e.response)\n",
    "        return e\n",
    "\n",
    "# Custom batch processing job\n",
    "async def batch_processing_job(texts: List[str], custom_class: Type[BaseModel]):\n",
    "\n",
    "    # Create a list of tasks, one for each input\n",
    "    tasks = [\n",
    "        backoff_retry_async(partial(extract_entities_from_single_input_async, text=text, custom_class=custom_class, client=client))\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    # Run tasks concurrently\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    # Process results\n",
    "    for idx, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"### Error encountered while processing record {idx + 1}: {result}\")\n",
    "        else:\n",
    "            print(f\"### Success for record {idx + 1}:\")\n",
    "            print(json.dumps(result.model_dump(), indent=2))\n",
    "\n",
    "# Function to load texts from a .jsonl file\n",
    "def load_texts_from_jsonl(file_path: str) -> List[str]:\n",
    "    texts = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            texts.append(data['content'])  # Extracting 'content' from each line\n",
    "    return texts\n",
    "\n",
    "# Load texts from the .jsonl file and run the extraction\n",
    "research_papers = load_texts_from_jsonl(\"sample_research_papers.jsonl\")\n",
    "await batch_processing_job(research_papers, ResearchPaperEntity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4a1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity-extraction-intro-xl0I0aMg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
